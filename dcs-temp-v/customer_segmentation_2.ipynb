{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faaabaa8",
   "metadata": {},
   "source": [
    "## Dynamic Customer Segmentation using LRFMS & Time Series Clustering\n",
    "This notebook implements the methodology described in the research paper 'A dynamic customer segmentation approach by combining LRFMS and multivariate time series clustering'. We will use a real-world transaction dataset to segment customers based on their behavior over time.\n",
    "\n",
    "###  Project Workflow:\n",
    "* **Setup & Data Loading:** Import libraries and load the datasets.\n",
    "\n",
    "* **Data Preparation:** Merge and clean the data to create an analytical base table.\n",
    "\n",
    "* **Time Series Aggregation:** Convert the transaction log into a time series format for each customer.\n",
    "\n",
    "LRFMS Feature Engineering: Calculate the Length, Recency, Frequency, Monetary, and Satisfaction metrics for each time period.\n",
    "\n",
    "Multivariate Time Series Clustering: Apply clustering algorithms to segment the customers.\n",
    "\n",
    "Analysis & Visualization: Analyze the resulting clusters and visualize their behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92416757",
   "metadata": {},
   "source": [
    "Step 1: Setup & Data Loading\n",
    "Explanation\n",
    "Before we begin, we need to set up our environment. This involves two parts:\n",
    "\n",
    "Installing Libraries: We need tslearn, a powerful library specifically designed for time series analysis and clustering in Python. We install it quietly using !pip install tslearn -q.\n",
    "\n",
    "Importing Packages: We import pandas and numpy for data manipulation, plotly for creating interactive visualizations, and key components from sklearn and tslearn for our machine learning tasks.\n",
    "\n",
    "Finally, we load the three CSV files (Transactions_Cleaned.csv, CustomerDemographic_Cleaned.csv, CustomerAddress_Cleaned.csv) into pandas DataFrames, which are essentially tables that make the data easy to work with. A try-except block is used to handle potential errors if the files aren't found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c81f67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      + meson setup C:\\Users\\satav\\AppData\\Local\\Temp\\pip-install-yvdf663g\\scikit-learn_633b5dfc37ae4616bd10697c648d57c0 C:\\Users\\satav\\AppData\\Local\\Temp\\pip-install-yvdf663g\\scikit-learn_633b5dfc37ae4616bd10697c648d57c0\\.mesonpy-q5cdp0xk -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\satav\\AppData\\Local\\Temp\\pip-install-yvdf663g\\scikit-learn_633b5dfc37ae4616bd10697c648d57c0\\.mesonpy-q5cdp0xk\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.9.1\n",
      "      Source dir: C:\\Users\\satav\\AppData\\Local\\Temp\\pip-install-yvdf663g\\scikit-learn_633b5dfc37ae4616bd10697c648d57c0\n",
      "      Build dir: C:\\Users\\satav\\AppData\\Local\\Temp\\pip-install-yvdf663g\\scikit-learn_633b5dfc37ae4616bd10697c648d57c0\\.mesonpy-q5cdp0xk\n",
      "      Build type: native build\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      Project name: scikit-learn\n",
      "      Project version: 1.6.1\n",
      "      \n",
      "      ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\satav\\AppData\\Local\\Temp\\pip-install-yvdf663g\\scikit-learn_633b5dfc37ae4616bd10697c648d57c0\\.mesonpy-q5cdp0xk\\meson-logs\\meson-log.txt\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tslearn\n",
      "  Using cached tslearn-0.6.4-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting scikit-learn<1.7,>=1.3.2 (from tslearn)\n",
      "  Using cached scikit_learn-1.6.1.tar.gz (7.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall tslearn\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "%pip install tslearn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# For clustering and evaluation\n",
    "import tslearn\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# For visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# --- Load the Datasets ---\n",
    "try:\n",
    "    transactions_df = pd.read_csv('Transactions_Cleaned.csv')\n",
    "    demographics_df = pd.read_csv('CustomerDemographic_Cleaned.csv')\n",
    "    address_df = pd.read_csv('CustomerAddress_Cleaned.csv')\n",
    "    print(\"All datasets loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}. Make sure the CSVs are in the current directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd0b7a",
   "metadata": {},
   "source": [
    "Step 2: Data Preparation\n",
    "Explanation\n",
    "Our data is currently spread across three separate files. To analyze it effectively, we need to combine it into a single, master table using pd.merge().\n",
    "\n",
    "We will then select only the columns needed for our LRFM analysis (customer_id, transaction_date, list_price). This clean, focused table is our Analytical Base Table (ABT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc3097",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Merge datasets to get a master table\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m merged_df = \u001b[43mpd\u001b[49m.merge(transactions_df, demographics_df, on=\u001b[33m'\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m master_df = pd.merge(merged_df, address_df, on=\u001b[33m'\u001b[39m\u001b[33mcustomer_id\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Convert transaction_date to datetime\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Merge datasets to get a master table\n",
    "merged_df = pd.merge(transactions_df, demographics_df, on='customer_id', how='left')\n",
    "master_df = pd.merge(merged_df, address_df, on='customer_id', how='left')\n",
    "\n",
    "# Convert transaction_date to datetime\n",
    "master_df['transaction_date'] = pd.to_datetime(master_df['transaction_date'])\n",
    "\n",
    "# Create the final analytical base table for LRFM\n",
    "abt = master_df[['customer_id', 'transaction_date', 'list_price']]\n",
    "abt = abt.rename(columns={'list_price': 'monetary_value'})\n",
    "abt.dropna(inplace=True)\n",
    "\n",
    "print(\"Data preparation complete. Here is a preview of the Analytical Base Table:\")\n",
    "abt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348e924",
   "metadata": {},
   "source": [
    "Step 3: Time Series Aggregation\n",
    "Explanation\n",
    "The LRFM model analyzes behavior over time. We convert our transaction log into a time series by grouping data into monthly bins using .resample('M'). For each customer in each month, we calculate:\n",
    "\n",
    "frequency: The count of transactions.\n",
    "\n",
    "monetary: The sum of all purchase values.\n",
    "\n",
    "This gives us a time series view of F and M for every customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f700efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set transaction_date as the index for time-based operations\n",
    "abt_indexed = abt.set_index('transaction_date')\n",
    "\n",
    "# Group by customer_id and resample into monthly bins\n",
    "time_series_df = abt_indexed.groupby('customer_id').resample('M').agg(\n",
    "    frequency=('customer_id', 'size'),\n",
    "    monetary=('monetary_value', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Resampling creates a row for every month, even with no transactions.\n",
    "# Fill months with no activity with 0 for frequency and monetary.\n",
    "time_series_df[['frequency', 'monetary']] = time_series_df[['frequency', 'monetary']].fillna(0)\n",
    "\n",
    "print(\"Time series aggregation complete. Preview:\")\n",
    "time_series_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5efe2",
   "metadata": {},
   "source": [
    "Step 4: LRFM Feature Engineering\n",
    "\n",
    "Explanation\n",
    "\n",
    "With our data in a monthly time series, we can now engineer the final LRFM features. F and M are already done. We now calculate Length (L) and Recency (R'):\n",
    "\n",
    "\n",
    "\n",
    "Length (L): Measures customer tenure. It's the number of days between their first purchase and their last purchase within the current monthly period.\n",
    "\n",
    "Recency (R'): Measures recent activity. It's the number of days between the end of the month and their last purchase in that month.\n",
    "\n",
    "We calculate these and add them as new columns to complete our LRFM feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the very first purchase date for each customer\n",
    "first_purchase = abt.groupby('customer_id')['transaction_date'].min().reset_index()\n",
    "first_purchase.rename(columns={'transaction_date': 'first_purchase_date'}, inplace=True)\n",
    "\n",
    "# Calculate the last purchase date within each month for each customer\n",
    "# FIX: Using a more stable groupby().agg() approach to avoid version-specific errors.\n",
    "abt['transaction_month'] = abt['transaction_date'].dt.to_period('M').dt.start_time\n",
    "last_purchase_in_period = abt.groupby(['customer_id', 'transaction_month'])['transaction_date'].max().reset_index()\n",
    "last_purchase_in_period.rename(columns={'transaction_date': 'last_purchase_date', 'transaction_month': 'transaction_date'}, inplace=True)\n",
    "\n",
    "# Merge these helper calculations back into the main time series dataframe\n",
    "ts_final = pd.merge(time_series_df, first_purchase, on='customer_id', how='left')\n",
    "ts_final = pd.merge(ts_final, last_purchase_in_period, on=['customer_id', 'transaction_date'], how='left')\n",
    "\n",
    "# --- Calculate L and R' in days --- #\n",
    "# Forward-fill the last purchase date for months where there were no purchases\n",
    "ts_final['last_purchase_date'] = ts_final.groupby('customer_id')['last_purchase_date'].ffill()\n",
    "\n",
    "ts_final['length'] = (ts_final['last_purchase_date'] - ts_final['first_purchase_date']).dt.days\n",
    "ts_final['recency'] = (ts_final['transaction_date'].dt.to_period('M').dt.end_time - ts_final['last_purchase_date']).dt.days\n",
    "\n",
    "# Handle NaNs that arise (e.g. for the very first month) by filling them with 0\n",
    "ts_final.fillna(0, inplace=True)\n",
    "\n",
    "# Select final features for clustering\n",
    "features = ['length', 'recency', 'frequency', 'monetary']\n",
    "final_lrfm_df = ts_final[['customer_id', 'transaction_date'] + features]\n",
    "\n",
    "print(\"LRFM Feature Engineering complete. Final dataset preview:\")\n",
    "final_lrfm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df850a8",
   "metadata": {},
   "source": [
    "Step 5: Multivariate Time Series Clustering\n",
    "Explanation\n",
    "This is the core machine learning step. The tslearn library requires data in a 3D shape: (number_of_customers, number_of_timesteps, number_of_features). We use .pivot() and .reshape() to transform our table into this 3D array.\n",
    "\n",
    "Before clustering, we scale the data using TimeSeriesScalerMinMax. This is crucial because features like monetary can have large values that would otherwise dominate the algorithm. Scaling brings all features into a 0-to-1 range.\n",
    "\n",
    "We then find the optimal number of clusters (k) by calculating the Silhouette Score for several values of k. The highest score indicates the best-defined clusters. Finally, we run TimeSeriesKMeans with the optimal k to get our final segments. We use the \"dtw\" (Dynamic Time Warping) metric, which is excellent for comparing time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table to prepare for reshaping\n",
    "pivoted_df = final_lrfm_df.pivot(index='customer_id', columns='transaction_date', values=features)\n",
    "pivoted_df.fillna(0, inplace=True) # Fill NaNs for customers who joined later\n",
    "\n",
    "# Reshape data into a 3D NumPy array (n_customers, n_timesteps, n_features)\n",
    "n_customers = len(pivoted_df.index)\n",
    "n_timesteps = len(final_lrfm_df['transaction_date'].unique())\n",
    "n_features = len(features)\n",
    "data_array = pivoted_df.values.reshape(n_customers, n_timesteps, n_features)\n",
    "\n",
    "# Scale the features\n",
    "scaler = TimeSeriesScalerMinMax()\n",
    "scaled_data = scaler.fit_transform(data_array)\n",
    "print(f\"Data reshaped and scaled. Shape: {scaled_data.shape}\")\n",
    "\n",
    "\n",
    "# --- SPEED UP: Create a sample for finding the optimal k ---\n",
    "sample_size = 800\n",
    "# Ensure sample size is not larger than the number of customers\n",
    "if sample_size > n_customers:\n",
    "    sample_size = n_customers\n",
    "\n",
    "np.random.seed(42) # for reproducibility\n",
    "sample_indices = np.random.choice(n_customers, sample_size, replace=False)\n",
    "data_sample = scaled_data[sample_indices]\n",
    "print(f\"Created a sample of {sample_size} customers to find optimal k.\")\n",
    "\n",
    "\n",
    "# --- Find the optimal number of clusters (k) on the SAMPLE --- #\n",
    "print(\"\\nFinding optimal number of clusters (k) on the sample...\")\n",
    "silhouette_scores = {}\n",
    "# Reshape sample data for silhouette score evaluation (needs 2D)\n",
    "reshaped_sample_for_eval = data_sample.reshape(sample_size, -1) \n",
    "\n",
    "for k in range(2, 6): # Test k from 2 to 5 (reduced range for speed)\n",
    "    model = TimeSeriesKMeans(n_clusters=k, metric=\"dtw\", max_iter=5, random_state=42, n_jobs=-1)\n",
    "    labels = model.fit_predict(data_sample)\n",
    "    score = silhouette_score(reshaped_sample_for_eval, labels)\n",
    "    silhouette_scores[k] = score\n",
    "    print(f\"k={k}, Silhouette Score: {score:.4f}\")\n",
    "\n",
    "optimal_k = max(silhouette_scores, key=silhouette_scores.get)\n",
    "print(f\"\\nOptimal k found from sample: {optimal_k}\")\n",
    "\n",
    "\n",
    "# --- Final Clustering on the FULL dataset --- #\n",
    "print(f\"\\nRunning final clustering with k={optimal_k} on the full dataset...\")\n",
    "final_model = TimeSeriesKMeans(n_clusters=optimal_k, metric=\"dtw\", max_iter=10, random_state=42, n_jobs=-1)\n",
    "final_labels = final_model.fit_predict(scaled_data) # Use the full 'scaled_data'\n",
    "\n",
    "# Add cluster labels back to our customer list\n",
    "customer_clusters = pd.DataFrame({'customer_id': pivoted_df.index, 'cluster': final_labels})\n",
    "print(\"\\nClustering complete.\")\n",
    "customer_clusters.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9613eff",
   "metadata": {},
   "source": [
    "Step 6: Analysis & Visualization\n",
    "Explanation\n",
    "Now that we have assigned each customer to a cluster, the final step is to understand what these clusters represent.\n",
    "\n",
    "To interpret the clusters, we calculate the average value for each of the LRFM features within each cluster using groupby('cluster').mean(). This gives us a cluster summary table that reveals the defining characteristics of each segment (e.g., high monetary value, high frequency).\n",
    "\n",
    "To bring this to life, we visualize the average monthly spend (monetary) for each cluster over time. This line chart is the most powerful output of our project, as it clearly shows the different behavioral patterns of our customer segments, allowing us to give them meaningful names like \"Loyal High Spenders\" or \"At-Risk Customers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cluster labels with the full LRFM time series data\n",
    "analysis_df = pd.merge(final_lrfm_df, customer_clusters, on='customer_id')\n",
    "\n",
    "# --- Visualize the Cluster Trends --- #\n",
    "# Calculate the average monetary value for each cluster over time\n",
    "cluster_trends = analysis_df.groupby(['cluster', 'transaction_date'])['monetary'].mean().reset_index()\n",
    "\n",
    "fig = px.line(\n",
    "    cluster_trends, \n",
    "    x='transaction_date', \n",
    "    y='monetary', \n",
    "    color='cluster', \n",
    "    title='Average Monthly Spend per Customer Cluster',\n",
    "    labels={'monetary': 'Average Monetary Value', 'transaction_date': 'Date', 'cluster': 'Customer Cluster'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title='Month',\n",
    "    yaxis_title='Average Spend',\n",
    "    legend_title='Cluster',\n",
    "    font=dict(family=\"Arial, sans-serif\", size=12)\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# --- Analyze Cluster Characteristics --- #\n",
    "# Calculate the average LRFM values and customer count for each cluster\n",
    "cluster_summary = analysis_df.groupby('cluster')[features].mean().round(2)\n",
    "cluster_summary['customer_count'] = analysis_df.groupby('cluster')['customer_id'].nunique()\n",
    "\n",
    "print(\"\\n--- Cluster Summary Statistics (Average LRFM values) ---\")\n",
    "cluster_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c87b067",
   "metadata": {},
   "source": [
    "Step 7: Forecasting Future Customer Spend\n",
    "Explanation\n",
    "Now that we understand the historical behavior of our customer segments, the next logical step is to predict their future value. We will use a forecasting model to predict the average monthly spend for each cluster for the next 6 months.\n",
    "\n",
    "For this task, we'll use Prophet, a powerful and user-friendly forecasting library developed by Facebook. It's excellent at handling time series data with trends and seasonality.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "Install Prophet: Add the necessary library.\n",
    "\n",
    "Loop Through Each Cluster: For each customer segment, we will train a separate forecasting model.\n",
    "\n",
    "Prepare Data: Prophet requires the data to be in a specific format with two columns: ds (for the date) and y (for the value we want to predict).\n",
    "\n",
    "Train and Predict: We'll train the model on the historical spending data and then ask it to predict the next 6 months.\n",
    "\n",
    "Visualize Forecast: We will combine the historical data with the new forecast and plot it, including an \"uncertainty interval\" which shows the likely range of future spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33462e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the forecasting library. You may need to restart the kernel after this.\n",
    "!pip install prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "\n",
    "# --- Create a plot for each cluster's forecast ---\n",
    "print(\"\\n--- Forecasting Future Spend for Each Cluster ---\")\n",
    "\n",
    "# Get the unique cluster labels\n",
    "clusters = analysis_df['cluster'].unique()\n",
    "clusters.sort()\n",
    "\n",
    "for cluster_id in clusters:\n",
    "    # Get the historical data for this specific cluster\n",
    "    cluster_data = cluster_trends[cluster_trends['cluster'] == cluster_id]\n",
    "    \n",
    "    # 1. Prepare data for Prophet\n",
    "    prophet_df = cluster_data[['transaction_date', 'monetary']].rename(columns={\n",
    "        'transaction_date': 'ds',\n",
    "        'monetary': 'y'\n",
    "    })\n",
    "    \n",
    "    # 2. Train the Prophet model\n",
    "    model = Prophet()\n",
    "    model.fit(prophet_df)\n",
    "    \n",
    "    # 3. Create a dataframe for future predictions (next 6 months)\n",
    "    future = model.make_future_dataframe(periods=6, freq='M')\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # 4. Visualize the forecast\n",
    "    fig = model.plot(forecast, xlabel='Date', ylabel='Average Spend')\n",
    "    ax = fig.gca()\n",
    "    ax.set_title(f'Forecasted Average Spend for Cluster {cluster_id}', size=20)\n",
    "    ax.set_xlabel(\"Month\", size=15)\n",
    "    ax.set_ylabel(\"Average Spend\", size=15)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
